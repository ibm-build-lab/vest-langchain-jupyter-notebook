{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "032112c4",
   "metadata": {},
   "source": [
    "## 1. Programmatically using WatsonX.ai models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07bfa6a-f8ae-4630-b123-65e6c549c627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add API key below\n",
    "\n",
    "%env API_KEY=\n",
    "%env IBM_CLOUD_URL=https://us-south.ml.cloud.ibm.com\n",
    "%env PROJECT_ID=925037f0-9d19-4918-b1aa-5e257e462fb6\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adcdb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import the dependencies we need:\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from time import sleep\n",
    "try:\n",
    "    from langchain import PromptTemplate\n",
    "    from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "    from langchain.document_loaders import PyPDFLoader\n",
    "    from langchain.indexes import VectorstoreIndexCreator # Vectorize db index with chromadb\n",
    "    from langchain.embeddings import HuggingFaceEmbeddings # For using HuggingFace embedding models\n",
    "    from langchain.text_splitter import CharacterTextSplitter # Text splitter\n",
    "\n",
    "    from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "    from ibm_watson_machine_learning.foundation_models import Model\n",
    "    from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "except ImportError as e:\n",
    "    print(e)\n",
    "\n",
    "print(\"Done importing dependencies.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96339420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our API key and URL from .env\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\", None)\n",
    "ibm_cloud_url = os.getenv(\"IBM_CLOUD_URL\", None)\n",
    "project_id = os.getenv(\"PROJECT_ID\", None)\n",
    "\n",
    "if api_key is None or ibm_cloud_url is None or project_id is None:\n",
    "    raise Exception(\"One or more environment variables are missing!\")\n",
    "else:\n",
    "    creds = {\n",
    "        \"url\": ibm_cloud_url,\n",
    "        \"apikey\": api_key \n",
    "    }\n",
    "print(\"Done getting env variables.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51cbd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the WatsonX model\n",
    "params = {\n",
    "    GenParams.DECODING_METHOD: \"sample\",\n",
    "    GenParams.TEMPERATURE: 0.2,\n",
    "    GenParams.TOP_P: 1,\n",
    "    GenParams.TOP_K: 25,\n",
    "    GenParams.REPETITION_PENALTY: 1.0,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.MAX_NEW_TOKENS: 20\n",
    "}\n",
    "\n",
    "llm_model = Model(\n",
    "    model_id=\"google/flan-ul2\",\n",
    "    params=params,\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ")\n",
    "print(\"Done initializing LLM.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b25c008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with the model\n",
    "countries = [\"France\", \"Japan\", \"Australia\"]\n",
    "\n",
    "try:\n",
    "  for country in countries:\n",
    "    question = f\"What is the capital of {country}\"\n",
    "    res = llm_model.generate_text(question)\n",
    "    print(f\"The capital of {country} is {res.capitalize()}\")\n",
    "except Exception as e:\n",
    "  print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52e2e8f",
   "metadata": {},
   "source": [
    "## 2. Prompt Templates & Chains\n",
    "\n",
    "In the previous example, the user input is sent directly to the Watsonx LLM, without using Langchain. This is a basic use case, but real applications are rarely so simple. When using an LLM in an application, you will usually need to reuse the same prompt across multiple scenarios. We will now replicate the previous example, but use an LLM chain. This allows us to:\n",
    "\n",
    "- Accept user input and contruct a prompt\n",
    "- Generate multiple prompts from a collection of data points in a dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310c2bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template\n",
    "prompt = PromptTemplate(\n",
    "  input_variables=[\"country\"],\n",
    "  template= \"What is the capital of {country}?\",\n",
    ")\n",
    "\n",
    "try:\n",
    "  # In order to use Langchain, we need to instantiate Langchain extension\n",
    "  lc_llm_model = WatsonxLLM(model=llm_model)\n",
    "  \n",
    "  # Define a chain based on model and prompt\n",
    "  chain = LLMChain(llm=lc_llm_model, prompt=prompt)\n",
    "\n",
    "  # Getting predictions\n",
    "  countries = [\"Sweden\", \"Mexico\", \"Vietnam\"]\n",
    "  for country in countries:\n",
    "    response = chain.run(country)\n",
    "    print(prompt.format(country=country) + \" = \" + response.capitalize())\n",
    "    sleep(0.5)\n",
    "except Exception as e:\n",
    "  print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918a9df3",
   "metadata": {},
   "source": [
    "## 3. Simple sequential chains\n",
    "The utility of LangChain becomes apparent as we chain outputs of one model as input to another model. Here's a simple example where one generates a question which the other model answers.\n",
    "\n",
    "LangChain determines a model's output based on its response.  In our examples, the first model creates a response to the end prompt of \"Question:\" which LangChain maps as an input variable called \"question\" which it passes to the 2nd model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffda7c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two sequential prompts \n",
    "pt1 = PromptTemplate(input_variables=[\"topic\"], template=\"Generate a random question about {topic}: Question: \")\n",
    "pt2 = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Answer the following question: {question}\",\n",
    ")\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e4e1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate 2 models (Note, these could be different models depending on use case)\n",
    "# Note the .to_langchain() method which returns a WatsonxLLM wrapper, like above.\n",
    "model_1 = Model(\n",
    "    model_id=\"google/flan-ul2\",\n",
    "    params=params,\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ").to_langchain()\n",
    "model_2 = Model(\n",
    "    model_id=\"google/flan-ul2\",\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ").to_langchain()\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35de1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the sequential chain\n",
    "prompt_to_model_1 = LLMChain(llm=model_1, prompt=pt1)\n",
    "prompt_to_model_2 = LLMChain(llm=model_2, prompt=pt2)\n",
    "qa = SimpleSequentialChain(chains=[prompt_to_model_1, prompt_to_model_2], verbose=True)\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34586549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run our chain with the topic: \"an animal\"\n",
    "# Play around with providing different topics to see the output. eg. cars, the Roman empire\n",
    "try:\n",
    "  qa.run(\"an animal\")\n",
    "except Exception as e:\n",
    "  print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7c152d",
   "metadata": {},
   "source": [
    "## 4. Easy Loading of Documents Using Lang Chain\n",
    "LangChain makes it easy to extract passages from documents so that you can answer questions based on your document's content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1743ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF document\n",
    "pdf='what-is-generative-ai.pdf'\n",
    "loaders = [PyPDFLoader(pdf)]\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9348f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index loaded PDF\n",
    "index = VectorstoreIndexCreator(\n",
    "    embedding=HuggingFaceEmbeddings(),\n",
    "    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)).from_loaders(loaders)\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a890f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize watsonx google/flan-ul2 model\n",
    "params = {\n",
    "    GenParams.DECODING_METHOD: \"sample\",\n",
    "    GenParams.TEMPERATURE: 0.2,\n",
    "    GenParams.TOP_P: 1,\n",
    "    GenParams.TOP_K: 100,\n",
    "    GenParams.MIN_NEW_TOKENS: 50,\n",
    "    GenParams.MAX_NEW_TOKENS: 300\n",
    "}\n",
    "model = Model(\n",
    "    model_id=\"google/flan-ul2\",\n",
    "    params=params,\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ").to_langchain()\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1a50d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init RAG chain\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm=model, \n",
    "                                    chain_type=\"stuff\", \n",
    "                                    retriever=index.vectorstore.as_retriever(), \n",
    "                                    input_key=\"question\")\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6efed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer based on the document\n",
    "res = chain.run(\"what is Machine Learning?\")\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2679f2",
   "metadata": {},
   "source": [
    "Retrieval Augmented Generation (RAG) is a common AI use case. Many companies have vast amounts of data about which they want an AI system to answer questions, do searches or perform summarization tasks. We will learn more about RAG in lab 106."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
