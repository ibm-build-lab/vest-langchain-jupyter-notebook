{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"setup\"></a>\n",
    "## LangChain memory with watsonx.ai models\n",
    "\n",
    "This notebook contains sample code for using *ConversationBufferMemory* with models included in watsonx.ai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Define the WML credentials\n",
    "This cell defines the WML credentials required to work with watsonx Foundation Model inferencing.\n",
    "\n",
    "**Action:** Provide the IBM Cloud user API key. For details, see\n",
    "[documentation](https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "# Set your region here: us-east, us-south, eu-de, etc\n",
    "REGION = \"us-south\"\n",
    "\n",
    "credentials = {\n",
    "    \"url\": \"https://\" + REGION + \".ml.cloud.ibm.com\",\n",
    "    \"apikey\": getpass.getpass(\"Please enter your WML api key (hit enter): \")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Define the project id\n",
    "The Foundation Model requires project id that provides the context for the call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set your project id here\n",
    "project_id = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import DecodingMethods\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to change model types, you can look them up with this line\n",
    "print([model.name for model in ModelTypes])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the LangChain model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the flan model\n",
    "\n",
    "# Experiment with different models. We noticed that Flan produces a more concise ouput, but llama - more descriptive and better quality\n",
    "#model_id_1 = \"meta-llama/llama-2-70b-chat\"\n",
    "model_id_1 = ModelTypes.FLAN_UL2\n",
    "\n",
    "model1_parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n",
    "    GenParams.MAX_NEW_TOKENS: 300,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P: 1\n",
    "}\n",
    "\n",
    "# WML model objects - will be used to create models for the LangChain API\n",
    "\n",
    "current_model = Model(\n",
    "    model_id=model_id_1, \n",
    "    params=model1_parameters, \n",
    "    credentials=credentials,\n",
    "    project_id=project_id)\n",
    "\n",
    "# Create the model objects that will be used by LangChain\n",
    "current_llm = WatsonxLLM(model=current_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the memory object\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=current_llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run these lines if you want to clear the memory without stopping the notebook (use if you provide different user input in the cell below)\n",
    "#memory.clear()\n",
    "#print(memory.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a first quesiton from the user\n",
    "user_input = \"From the following customer complaint, extract 3 factors that caused the customer to be unhappy. \\\n",
    "                            Put each factor on a new line. Customer complaint: I am writing you this statement to delete the \\\n",
    "                            following information on my credit report. The items I need deleted are listed in the report. \\\n",
    "                            I am a victim of identity thief, I demand that you remove these errors to correct my report immediately! \\\n",
    "                            I have reported this to the federal trade commission and have attached the federal trade commission affidavit. \\\n",
    "                            Now that I have given you the following information, please correct my credit report or I shall proceed with involving my attorney! \\\n",
    "                            Numbered list of complaints:\"\n",
    "\n",
    "#user_input=\"Right now I am bothered! I have attempted to be patient however it is hard to be patient when you feel that you are continually being overlooked by somebody. I think you fail to remember that \\Consumer detailing organizations have expected an essential part in amassing and assessing customer credit and other data on shoppers. The XXXX XXXX  is reliant on the reasonable and precision.\"\n",
    "\n",
    "# Invoke the LLM\n",
    "conversation.predict(input=user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a second quesiton from the user\n",
    "user_input = \"Does the numbered list of complaints contain a statement about identity fraud? Provide a short answer: yes or no.\"\n",
    "conversation.predict(input=user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging, print the history of the conversation\n",
    "# print(type(memory.buffer_as_messages))\n",
    "messages = memory.buffer_as_messages\n",
    "for i in messages:\n",
    "    line = ' '.join(i.content.split()).replace(\"Customer complaint:\", '\\n\\nCustomer complaint:').replace(\"Numbered list of complaints:\", '\\n\\nNumbered list of complaints:')\n",
    "    print(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Authors: \n",
    " **Elena Lowery**, Data and AI Architect"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
