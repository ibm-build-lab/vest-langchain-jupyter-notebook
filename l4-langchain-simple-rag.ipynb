{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring in dependencies\n",
    "\n",
    "In this code cell we'll bring in all the dependencies we'll need for later use.\n",
    "\n",
    "Go ahead and run the following code cell. **There should be no ouput.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.document_loaders.pdf import PyPDFLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# WML python SDK\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes, DecodingMethods\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Some important variables\n",
    "\n",
    "In this next code cell you'll define some variables that will be used in order to interact with your instance of watsonx.ai.\n",
    "\n",
    "Go ahead and run the following code cell. **There should be no ouput**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the global variables that will be used for authentication in another function\n",
    "\n",
    "# Set your project id here\n",
    "watsonx_project_id = \"\"\n",
    "\n",
    "# API key here\n",
    "api_key = \"\"\n",
    "\n",
    "# Set your region here: us-south, us-east, eu-de, etc.\n",
    "REGION = \"us-south\"\n",
    "\n",
    "url = \"https://\" + REGION + \".ml.cloud.ibm.com\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the code\n",
    "\n",
    "In this next code cell we'll create some functions that we can use later to interact easier with watsonx.ai. These functions are ***get_model()***, ***get_lang_chain_model()***, and ***answer_question_from_doc()***:\n",
    "\n",
    "- ***get_model()***: creates a model object that will be used to invoke the LLM. Since the ***get_model()*** function is parametrized, it's the same in all examples.\n",
    "- ***get_lang_chain_model()***: creates a model wrapper that will be used with the _LangChain_ API.\n",
    "- ***answer_question_from_doc()*** specifies model parameters, loads the PDF file, creates an index from the loaded document, the instantiates and invokes the chain.\n",
    "\n",
    "Go ahead and run the following code cell. **There should be no ouput**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_type,max_tokens,min_tokens,decoding,temperature):\n",
    "\n",
    "    generate_params = {\n",
    "        GenParams.MAX_NEW_TOKENS: max_tokens,\n",
    "        GenParams.MIN_NEW_TOKENS: min_tokens,\n",
    "        GenParams.DECODING_METHOD: decoding,\n",
    "        GenParams.TEMPERATURE: temperature\n",
    "    }\n",
    "\n",
    "    model = Model(\n",
    "        model_id=model_type,\n",
    "        params=generate_params,\n",
    "        credentials={\n",
    "            \"apikey\": api_key,\n",
    "            \"url\": url\n",
    "        },\n",
    "        project_id=watsonx_project_id\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_lang_chain_model(model_type,max_tokens,min_tokens,decoding,temperature):\n",
    "\n",
    "    base_model = get_model(model_type,max_tokens,min_tokens,decoding,temperature)\n",
    "    langchain_model = WatsonxLLM(model=base_model)\n",
    "\n",
    "    return langchain_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gluing it together\n",
    "\n",
    "The next function, `answer_questions_from_doc`, that we create is created to help combine the previous three that we defined. This is the wrapper that we will call when we want to interact with watsonx.ai.\n",
    "\n",
    "Go ahead and run the following code cell. **There should be no ouput**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_questions_from_doc(file_path, question):\n",
    "\n",
    "  # Specify model parameters\n",
    "  model_type = \"meta-llama/llama-2-70b-chat\"\n",
    "  max_tokens = 300\n",
    "  min_tokens = 100\n",
    "  decoding = DecodingMethods.GREEDY\n",
    "  temperature = 0.7\n",
    "\n",
    "  # Get the watsonx model that can be used with LangChain\n",
    "  model = get_lang_chain_model(model_type, max_tokens, min_tokens, decoding, temperature)\n",
    "\n",
    "  loaders = [PyPDFLoader(file_path)]\n",
    "\n",
    "  index = VectorstoreIndexCreator(\n",
    "      embedding=HuggingFaceEmbeddings(),\n",
    "      text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)).from_loaders(loaders)\n",
    "\n",
    "  chain = RetrievalQA.from_chain_type(llm=model,\n",
    "                                      chain_type=\"stuff\",\n",
    "                                      retriever=index.vectorstore.as_retriever(),\n",
    "                                      input_key=\"question\")\n",
    "\n",
    "  # Invoke the chain\n",
    "  response_text = chain.invoke(question)\n",
    "\n",
    "  # print model response\n",
    "  print(\"--------------------------------- Generated response -----------------------------------\")\n",
    "  print(response_text[\"result\"])\n",
    "  print(\"*********************************************************************************************\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering some questions\n",
    "\n",
    "The next code cell will use all the previous code we've created so far to source information from the input documents and ask a question about them using watsonx.ai (Notice the use of the `answer_questions_from_doc`).\n",
    "\n",
    "To do so we'll pass in a question we want to ask, the PDF file we want to reference for said question, and finally the name of the collection where the embeddings of the file exist.\n",
    "\n",
    "Notice the commented questions as well? Feel free to uncomment these or create some or your own to ask\n",
    "\n",
    "Go ahead and run the next code cell. **You _will_ see output from this cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test answering questions based on the provided .pdf file\n",
    "question = \"What is Generative AI?\"\n",
    "# question = \"What does it take to build a generative AI model?\"\n",
    "# question = \"What are the limitations of generative AI models?\"\n",
    "file_path = \"https://raw.githubusercontent.com/CloudPak-Outcomes/Outcomes-Projects/main/L4assets/watsonx.ai-Assets/Documents/Generative_AI_Overview.pdf\"\n",
    "\n",
    "answer_questions_from_doc(file_path, question)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
